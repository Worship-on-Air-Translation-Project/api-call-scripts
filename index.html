<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Live Translation</title>
  <style>
    body {
      margin: 0;
      padding: 0;
      background: #f9f9f9;
      font-family: sans-serif;
      height: 100vh;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    .wrapper {
      text-align: center;
      width: 90%;
      max-width: 800px;
    }

    h1 {
      color: #0ea5e9;
    }

    #output {
      margin-top: 30px;
      background: #e2e8f0;
      padding: 16px;
      border-radius: 10px;
      height: 400px;
      overflow-y: auto;
      text-align: left;
    }

    .pair {
      margin-bottom: 20px; /* space between transcript+translation blocks */
      padding-bottom: 10px;
      border-bottom: 1px solid #cbd5e1;
    }

    .transcript {
      font-weight: bold;
      margin-bottom: 6px;
      color: #111827;
    }

    .translation {
      color: #334155;
    }
  </style>
</head>
<body>
  <div class="wrapper">
    <h1>Worship On Air - AI Translation</h1>
    <p><em>English ‚Üí Korean</em></p>

    <div id="output"></div>
  </div>

<!-- Azure Speech SDK (browser) -->
<script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>

<button id="startBtn" style="margin-top:16px;">üéôÔ∏è Start Microphone</button>
<div id="status" style="margin-top:8px;color:#334155;"></div>

<script>
const outputEl = document.getElementById("output");
const statusEl = document.getElementById("status");
const startBtn = document.getElementById("startBtn");

function setStatus(msg, err=false){ statusEl.textContent = msg; statusEl.style.color = err ? "#b91c1c" : "#334155"; }
function addPair(src, dst){
  const wrap = document.createElement("div");
  wrap.className = "pair";
  const t1 = document.createElement("div"); t1.className = "transcript";  t1.textContent = src || "";
  const t2 = document.createElement("div"); t2.className = "translation"; t2.textContent = dst || "";
  wrap.appendChild(t1); wrap.appendChild(t2);
  outputEl.appendChild(wrap);
  outputEl.scrollTop = outputEl.scrollHeight;
}

// 1) Everyone subscribes to the shared stream
const scheme = location.protocol === "https:" ? "wss" : "ws";
const ws = new WebSocket(`${scheme}://${location.host}/ws/translate`);
ws.onopen  = () => setStatus("WS connected");
ws.onerror = () => setStatus("WS error", true);
ws.onmessage = (ev) => {
  try {
    const { transcript, translation } = JSON.parse(ev.data);
    if (transcript || translation) addPair(transcript, translation);
  } catch {}
};

// 2) Mic + STT in the browser (Chrome), then POST text to /publish
startBtn.addEventListener("click", async () => {
  try {
    if (!window.isSecureContext) setStatus("‚ö†Ô∏è Use HTTPS for mic access", true);

    // Prompt Chrome for mic permission once
    const probe = await navigator.mediaDevices.getUserMedia({
      audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: false }
    });
    probe.getTracks().forEach(t => t.stop());

    // Get region + token from backend (so region isn‚Äôt hard-coded)
    const { region } = await fetch("/speech/config").then(r => r.json());
    const token = await fetch("/speech/token", { method: "POST" }).then(r => r.text());
    if (!region || !token) return setStatus("Missing region/token", true);

    // Init Azure Speech SDK
    const speechConfig = SpeechSDK.SpeechConfig.fromAuthorizationToken(token, region);
    speechConfig.speechRecognitionLanguage = "en-US";  // set your source language
    const audioConfig  = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
    const recognizer   = new SpeechSDK.SpeechRecognizer(speechConfig, audioConfig);

    // Keep token fresh (~10min expiry)
    setInterval(async () => {
      try {
        recognizer.authorizationToken = await fetch("/speech/token", { method: "POST" }).then(r => r.text());
      } catch {}
    }, 9 * 60 * 1000);

    recognizer.recognizing = (_s, e) => setStatus("Listening‚Ä¶ " + (e.result?.text || ""));
    recognizer.canceled    = (_s, e) => setStatus("Canceled: " + e.errorDetails, true);
    recognizer.sessionStopped = () => setStatus("Stopped");

    // On each final result, POST to server ‚Üí translate ‚Üí broadcast
    recognizer.recognized = async (_s, e) => {
      const text = e.result?.text?.trim();
      if (!text) return;
      await fetch("/publish", {
        method: "POST",
        headers: {"Content-Type":"application/json"},
        body: JSON.stringify({ text, from: "en", to: "ko" })  // adjust target
      });
    };

    recognizer.startContinuousRecognitionAsync(
      () => setStatus("üé§ Mic active. Speak now."),
      (err) => setStatus("Start error: " + err, true)
    );
  } catch (e) {
    setStatus("Mic/SDK error: " + e.message, true);
  }
});
</script>

</body>
</html>